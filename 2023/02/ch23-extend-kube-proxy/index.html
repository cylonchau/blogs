<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？ | Cylon's Collection</title><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NP3JNCPR" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><meta name=keywords content="kubernetes,kubernetes,service,kube-proxy"><meta name=description content="本文是关于Kubernetes service解析的第四章
深入理解Kubernetes service - 你真的理解service吗? 深入理解Kubernetes service - EndpointSlices做了什么？ 深入理解Kubernetes service - kube-proxy架构分析 深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？ 所有关于Kubernetes service 部分代码上传至仓库 github.com/cylonchau/kube-haproxy
Overview 在前两部分中，学习了一些 service,于kube-proxy在设计架构，但存在扩展问题将引入了一些问题：
为什么需要了解这部分内容呢？ 与传统架构有什么区别呢？ 于eBPF 的 cilium又有什么区别呢？ 既然eBPF可以做到，那为什么要这部分内容呢？ 接下来的内容将围绕这四个问题展开来讲，而不是代码的讲解，代码可以看置顶
IPVS与iptables在kubernetes中应用时的问题 对于在使用了kubernetes用户以及了解 kube-proxy 架构后，知道当集群规模过大时，service必将增多，而一个service未必是一条iptables/ipvs规则，对于kubernetes这种分布式架构来说，集群规模越大，集群状态就越不可控，尤其时kube-proxy。
为什么单指kube-proxy呢？想想可以知道，pod的故障 或 node 的故障对于kubernetes集群来说却不是致命的，因为 这些资源集群中存在 避免方案，例如Pod的驱逐。而kube-proxy或iptables/IPVS问题将导致服务的不可控 『抖动』例如规则生成的快慢和Pod就绪的快慢不一致，部分节点不存在 service 此时服务必然抖动。
再例如 iptables/IPVS 排查的难度对于普通运维工程师或开发工程师的技术水平有很高的要求，网上随处可见分析该类问题的帖子：
kube-proxy源码分析与问题定位
案例分析：怎么解决海量IPVS规则带来的网络延时抖动问题？
ipvs 连接复用引发的系列问题
Investigating Causes of Jitter in Container Networking
ContainerNative network LoadBalancer IPVS jitter
对于上述问题，相信遇到了很难定位处理，虽然现在已fixed，并有eBPF技术的加入减少了此类问题的发生，但是eBPF实际同理于IPVS 都是需要对Linux内核有一定了解后才可以，这也就是为什么需要了解这部分
如果需要自定义proxier为什么会解决这个问题 这里就是放大到kubernetes意外的传统架构中，当直接部署于Linux系统上使用nginx等传统LB时就很少有人提到这些问题了，而这些问题存在一个关键字「Container」；而引发这个问题的则是 service。去除 service 的功能，传统架构于Kubernetes架构部署的应用则是相同的，只是区分了名称空间。"><meta name=author content="cylon"><link rel=canonical href=https://www.oomkill.com/2023/02/ch23-extend-kube-proxy/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://www.oomkill.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.oomkill.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.oomkill.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.oomkill.com/favicon.ico><link rel=mask-icon href=https://www.oomkill.com/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://www.oomkill.com/2023/02/ch23-extend-kube-proxy/><noscript><style>#theme-toggle,#top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link crossorigin=anonymous href=/assets/css/pe.min.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/pe.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/v4-shims.min.css><script defer src=https://cdn.staticfile.net/jquery/3.5.1/jquery.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/fancybox/3.5.7/jquery.fancybox.min.css><script defer src=https://cdn.staticfile.net/fancybox/3.5.7/jquery.fancybox.min.js></script><script id=MathJax-script async src=https://cdn.staticfile.net/mathjax/3.2.2/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["$$","$$"]],inlineMath:[["\\$","\\$"]]}}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-H94HZ5S19Y"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-H94HZ5S19Y")</script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><meta property="og:title" content="深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？"><meta property="og:description" content="本文是关于Kubernetes service解析的第四章
深入理解Kubernetes service - 你真的理解service吗? 深入理解Kubernetes service - EndpointSlices做了什么？ 深入理解Kubernetes service - kube-proxy架构分析 深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？ 所有关于Kubernetes service 部分代码上传至仓库 github.com/cylonchau/kube-haproxy
Overview 在前两部分中，学习了一些 service,于kube-proxy在设计架构，但存在扩展问题将引入了一些问题：
为什么需要了解这部分内容呢？ 与传统架构有什么区别呢？ 于eBPF 的 cilium又有什么区别呢？ 既然eBPF可以做到，那为什么要这部分内容呢？ 接下来的内容将围绕这四个问题展开来讲，而不是代码的讲解，代码可以看置顶
IPVS与iptables在kubernetes中应用时的问题 对于在使用了kubernetes用户以及了解 kube-proxy 架构后，知道当集群规模过大时，service必将增多，而一个service未必是一条iptables/ipvs规则，对于kubernetes这种分布式架构来说，集群规模越大，集群状态就越不可控，尤其时kube-proxy。
为什么单指kube-proxy呢？想想可以知道，pod的故障 或 node 的故障对于kubernetes集群来说却不是致命的，因为 这些资源集群中存在 避免方案，例如Pod的驱逐。而kube-proxy或iptables/IPVS问题将导致服务的不可控 『抖动』例如规则生成的快慢和Pod就绪的快慢不一致，部分节点不存在 service 此时服务必然抖动。
再例如 iptables/IPVS 排查的难度对于普通运维工程师或开发工程师的技术水平有很高的要求，网上随处可见分析该类问题的帖子：
kube-proxy源码分析与问题定位
案例分析：怎么解决海量IPVS规则带来的网络延时抖动问题？
ipvs 连接复用引发的系列问题
Investigating Causes of Jitter in Container Networking
ContainerNative network LoadBalancer IPVS jitter
对于上述问题，相信遇到了很难定位处理，虽然现在已fixed，并有eBPF技术的加入减少了此类问题的发生，但是eBPF实际同理于IPVS 都是需要对Linux内核有一定了解后才可以，这也就是为什么需要了解这部分
如果需要自定义proxier为什么会解决这个问题 这里就是放大到kubernetes意外的传统架构中，当直接部署于Linux系统上使用nginx等传统LB时就很少有人提到这些问题了，而这些问题存在一个关键字「Container」；而引发这个问题的则是 service。去除 service 的功能，传统架构于Kubernetes架构部署的应用则是相同的，只是区分了名称空间。"><meta property="og:type" content="article"><meta property="og:url" content="https://www.oomkill.com/2023/02/ch23-extend-kube-proxy/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-12T00:00:00+00:00"><meta property="article:modified_time" content="2023-02-12T23:00:36+08:00"><meta property="og:site_name" content="Cylon's Collection"><meta name=twitter:card content="summary"><meta name=twitter:title content="深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？"><meta name=twitter:description content="本文是关于Kubernetes service解析的第四章
深入理解Kubernetes service - 你真的理解service吗? 深入理解Kubernetes service - EndpointSlices做了什么？ 深入理解Kubernetes service - kube-proxy架构分析 深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？ 所有关于Kubernetes service 部分代码上传至仓库 github.com/cylonchau/kube-haproxy
Overview 在前两部分中，学习了一些 service,于kube-proxy在设计架构，但存在扩展问题将引入了一些问题：
为什么需要了解这部分内容呢？ 与传统架构有什么区别呢？ 于eBPF 的 cilium又有什么区别呢？ 既然eBPF可以做到，那为什么要这部分内容呢？ 接下来的内容将围绕这四个问题展开来讲，而不是代码的讲解，代码可以看置顶
IPVS与iptables在kubernetes中应用时的问题 对于在使用了kubernetes用户以及了解 kube-proxy 架构后，知道当集群规模过大时，service必将增多，而一个service未必是一条iptables/ipvs规则，对于kubernetes这种分布式架构来说，集群规模越大，集群状态就越不可控，尤其时kube-proxy。
为什么单指kube-proxy呢？想想可以知道，pod的故障 或 node 的故障对于kubernetes集群来说却不是致命的，因为 这些资源集群中存在 避免方案，例如Pod的驱逐。而kube-proxy或iptables/IPVS问题将导致服务的不可控 『抖动』例如规则生成的快慢和Pod就绪的快慢不一致，部分节点不存在 service 此时服务必然抖动。
再例如 iptables/IPVS 排查的难度对于普通运维工程师或开发工程师的技术水平有很高的要求，网上随处可见分析该类问题的帖子：
kube-proxy源码分析与问题定位
案例分析：怎么解决海量IPVS规则带来的网络延时抖动问题？
ipvs 连接复用引发的系列问题
Investigating Causes of Jitter in Container Networking
ContainerNative network LoadBalancer IPVS jitter
对于上述问题，相信遇到了很难定位处理，虽然现在已fixed，并有eBPF技术的加入减少了此类问题的发生，但是eBPF实际同理于IPVS 都是需要对Linux内核有一定了解后才可以，这也就是为什么需要了解这部分
如果需要自定义proxier为什么会解决这个问题 这里就是放大到kubernetes意外的传统架构中，当直接部署于Linux系统上使用nginx等传统LB时就很少有人提到这些问题了，而这些问题存在一个关键字「Container」；而引发这个问题的则是 service。去除 service 的功能，传统架构于Kubernetes架构部署的应用则是相同的，只是区分了名称空间。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.oomkill.com/posts/"},{"@type":"ListItem","position":2,"name":"深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？","item":"https://www.oomkill.com/2023/02/ch23-extend-kube-proxy/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？","name":"深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？","description":"本文是关于Kubernetes service解析的第四章\n深入理解Kubernetes service - 你真的理解service吗? 深入理解Kubernetes service - EndpointSlices做了什么？ 深入理解Kubernetes service - kube-proxy架构分析 深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？ 所有关于Kubernetes service 部分代码上传至仓库 github.com/cylonchau/kube-haproxy\nOverview 在前两部分中，学习了一些 service,于kube-proxy在设计架构，但存在扩展问题将引入了一些问题：\n为什么需要了解这部分内容呢？ 与传统架构有什么区别呢？ 于eBPF 的 cilium又有什么区别呢？ 既然eBPF可以做到，那为什么要这部分内容呢？ 接下来的内容将围绕这四个问题展开来讲，而不是代码的讲解，代码可以看置顶\nIPVS与iptables在kubernetes中应用时的问题 对于在使用了kubernetes用户以及了解 kube-proxy 架构后，知道当集群规模过大时，service必将增多，而一个service未必是一条iptables/ipvs规则，对于kubernetes这种分布式架构来说，集群规模越大，集群状态就越不可控，尤其时kube-proxy。\n为什么单指kube-proxy呢？想想可以知道，pod的故障 或 node 的故障对于kubernetes集群来说却不是致命的，因为 这些资源集群中存在 避免方案，例如Pod的驱逐。而kube-proxy或iptables/IPVS问题将导致服务的不可控 『抖动』例如规则生成的快慢和Pod就绪的快慢不一致，部分节点不存在 service 此时服务必然抖动。\n再例如 iptables/IPVS 排查的难度对于普通运维工程师或开发工程师的技术水平有很高的要求，网上随处可见分析该类问题的帖子：\nkube-proxy源码分析与问题定位\n案例分析：怎么解决海量IPVS规则带来的网络延时抖动问题？\nipvs 连接复用引发的系列问题\nInvestigating Causes of Jitter in Container Networking\nContainerNative network LoadBalancer IPVS jitter\n对于上述问题，相信遇到了很难定位处理，虽然现在已fixed，并有eBPF技术的加入减少了此类问题的发生，但是eBPF实际同理于IPVS 都是需要对Linux内核有一定了解后才可以，这也就是为什么需要了解这部分\n如果需要自定义proxier为什么会解决这个问题 这里就是放大到kubernetes意外的传统架构中，当直接部署于Linux系统上使用nginx等传统LB时就很少有人提到这些问题了，而这些问题存在一个关键字「Container」；而引发这个问题的则是 service。去除 service 的功能，传统架构于Kubernetes架构部署的应用则是相同的，只是区分了名称空间。","keywords":["kubernetes","kubernetes","service","kube-proxy"],"articleBody":" 本文是关于Kubernetes service解析的第四章\n深入理解Kubernetes service - 你真的理解service吗? 深入理解Kubernetes service - EndpointSlices做了什么？ 深入理解Kubernetes service - kube-proxy架构分析 深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？ 所有关于Kubernetes service 部分代码上传至仓库 github.com/cylonchau/kube-haproxy\nOverview 在前两部分中，学习了一些 service,于kube-proxy在设计架构，但存在扩展问题将引入了一些问题：\n为什么需要了解这部分内容呢？ 与传统架构有什么区别呢？ 于eBPF 的 cilium又有什么区别呢？ 既然eBPF可以做到，那为什么要这部分内容呢？ 接下来的内容将围绕这四个问题展开来讲，而不是代码的讲解，代码可以看置顶\nIPVS与iptables在kubernetes中应用时的问题 对于在使用了kubernetes用户以及了解 kube-proxy 架构后，知道当集群规模过大时，service必将增多，而一个service未必是一条iptables/ipvs规则，对于kubernetes这种分布式架构来说，集群规模越大，集群状态就越不可控，尤其时kube-proxy。\n为什么单指kube-proxy呢？想想可以知道，pod的故障 或 node 的故障对于kubernetes集群来说却不是致命的，因为 这些资源集群中存在 避免方案，例如Pod的驱逐。而kube-proxy或iptables/IPVS问题将导致服务的不可控 『抖动』例如规则生成的快慢和Pod就绪的快慢不一致，部分节点不存在 service 此时服务必然抖动。\n再例如 iptables/IPVS 排查的难度对于普通运维工程师或开发工程师的技术水平有很高的要求，网上随处可见分析该类问题的帖子：\nkube-proxy源码分析与问题定位\n案例分析：怎么解决海量IPVS规则带来的网络延时抖动问题？\nipvs 连接复用引发的系列问题\nInvestigating Causes of Jitter in Container Networking\nContainerNative network LoadBalancer IPVS jitter\n对于上述问题，相信遇到了很难定位处理，虽然现在已fixed，并有eBPF技术的加入减少了此类问题的发生，但是eBPF实际同理于IPVS 都是需要对Linux内核有一定了解后才可以，这也就是为什么需要了解这部分\n如果需要自定义proxier为什么会解决这个问题 这里就是放大到kubernetes意外的传统架构中，当直接部署于Linux系统上使用nginx等传统LB时就很少有人提到这些问题了，而这些问题存在一个关键字「Container」；而引发这个问题的则是 service。去除 service 的功能，传统架构于Kubernetes架构部署的应用则是相同的，只是区分了名称空间。\n抓入关键的核心之后就做接下来的事情了，我称之为「shed kube-proxy, fetch service」；即把service提取到集群外部的LB之上，例如F5, nginx等。\n这里会存在一个疑问：「这个不是ingress吗？」，这个问题会在下一章讲到 proxier与ingress有什么区别?\n软件的设计 既然拿到了核心问题就该定义软件工作模式，这里将软件架构设计为三种：\nonly fetch：任然需要 kube-proxy 组件，通过定义 contoller 将流量引入，即不过service，这种场景不会破坏现有的集群架构，从而去除service的功能，如果需要service功能配置外部service即可 SK (similar kube-proxy)：通过效仿kube-proxy + ipvs架构，将LB于proxier部署在每个worker节点上，让浏览都走本地 replacement kube-proxy：完全取代kube-proxy 这于cilium类似了，但不同的是，proxier 可以于 kube-controller-manager；kube-scheduler 作为控制平面为集群提供 service 功能，而无需为所有worker节点都部署一个 kube-proxy 或 cilium 这种架构 最后一个问题 此时可以引入最后一个问题了：「既然eBPF可以做到，那为什么要这部分内容呢？」。\n答：其一简单，每个运维人员无需额外知识都可以对 service 问题进行排错，简便了运维复杂度。另外这一部分其实是对于完整企业生态来讲，统一的流量转发平台是所必须的，有了这个就不需要单独的 service 功能了\n实践：基于haproxy的proxier 在扩展proxier需要对 kube-proxy 有一定的了解，并且，kube-proxy 在可扩展性来说做的也是相当不错的，我们只需要实现一个 proxier.go 就可以基本上完成了对 kube-proxy ；而 proxier.go 的核心方法只需要三个函数即可（==这里是根据iptables/ipvs的设计进行的，也可以整合为一个方法==）\n除了这三个函数外，其他的函数全都是 kube-proxy 已经实现好的通用的，这里直接使用或者按照其他内置proxier的方法即可\n满足条件 haproxy工作与proxier相同的节点，可以是集群内也可以是集群外，整个集群只需要一个 实现方法：syncProxyRules(), syncService(), syncEndpoint() 查看当前的service\nbash 1 2 3 4 $ kubectl get endpointslices NAME ADDRESSTYPE PORTS ENDPOINTS AGE kubernetes IPv4 6443 10.0.0.4 195d netbox-l489z IPv4 80 192.168.1.241,192.168.1.242 2d1h 查看service 配置\nyaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Service metadata: name: netbox spec: clusterIP: 192.168.129.5 ports: - port: 88 protocol: TCP targetPort: 80 selector: app: nginx sessionAffinity: None type: ClusterIP status: loadBalancer: {} 通过 proxier 生成了对应的 backend 与 frontend，这样就可以通过 haproxy 作为一个外部LB来跨过 service 与 IPVS/IPTables，通过这种情况下，我们可以将集群拉出一个平面至传统架构上，而又不影响集群的功能\n在这种场景下需要注意的是：\nOF模式下，我们需要 kube-proxy 组件，而使用 kube-proxy 组件 所有模式下，haproxy worker和kubernetes nodes需处于一个网络平面 非OF模式下需要自行修改 kube-apiserver 源代码（主要是使kubernetes service分配机制） Proxier与Ingress的区别 肯定有人会问，kubernetes提供了Ingress功能不是和这个一样吗？\n答：对比一个LB是Proxier还是Ingress最好的区别就是“舍去kube-proxy”可以工作正常吗？\n而kubernetes官方也提供说明，Ingress的后端是service，service的后端则是IPVS/IPTables，而IPVS的后端才是Pod；相对于Proxier LB，他的后端则直接是Pod，跨越了Service。\nKubernetes Ingress 架构说明 [1] Traefik Ingress 架构说明 [2] APISIX Ingress 架构说明 [3] 而相对的本文的学习思路，haproxy官方提供了对应的解决方案 [4] ；而由此，可以灵活的为Kubernetes提供更多的LB方案\nReference\n[1] Kubernetes Ingress 架构说明\n[2] Traefik Ingress 架构说明\n[3] APISIX Ingress 架构说明\n[4] External haproxy\n","wordCount":"261","inLanguage":"zh","datePublished":"2023-02-12T00:00:00Z","dateModified":"2023-02-12T23:00:36+08:00","author":{"@type":"Person","name":"cylon"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.oomkill.com/2023/02/ch23-extend-kube-proxy/"},"publisher":{"@type":"Organization","name":"Cylon's Collection","logo":{"@type":"ImageObject","url":"https://www.oomkill.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.oomkill.com/><img src=https://www.oomkill.com/favicon.ico alt aria-label=logo height=20>Cylon's Collection</a><div class=logo-switches><button id=theme-toggle><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.oomkill.com/archives><span>归档</span></a></li><li><a href=https://www.oomkill.com/tags><span>标签</span></a></li><li><a href=https://www.oomkill.com/search><span>搜索</span></a></li><li><a href=https://www.oomkill.com/about accesskey=/><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？</h1><div class=post-meta><span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>2023-02-12</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg><span>261 字</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><span>2 分钟</span></span>
<span class=pe-post-meta-item>&nbsp;·&nbsp;<svg t="1714036239378" fill="currentcolor" class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="6659" width="256" height="256"><path d="M690 78.2c-18.6-18.8-49-19-67.8-.4s-19 49-.4 67.8l255.4 258.6c67.8 68.6 67.8 178.8.0 247.4L653.4 878.2c-18.6 18.8-18.4 49.2.4 67.8s49.2 18.4 67.8-.4l224-226.4c104.8-106 104.8-276.4.0-382.4L690 78.2zM485.4 101.4c-24-24-56.6-37.4-90.6-37.4H96C43 64 0 107 0 160v299c0 34 13.4 66.6 37.4 90.6l336 336c50 50 131 50 181 0l267-267c50-50 50-131 0-181l-336-336zM96 160h299c8.4.0 16.6 3.4 22.6 9.4l336 336c12.4 12.4 12.4 32.8.0 45.2l-267 267c-12.4 12.4-32.8 12.4-45.2.0l-336-336c-6-6-9.4-14.2-9.4-22.6V160zm192 128a64 64 0 10-128 0 64 64 0 10128 0z" p-id="6660"/></svg></span><ul class=pe-post-meta-item><a href=https://www.oomkill.com/tags/kubernetes/>#Kubernetes</a>
<a href=https://www.oomkill.com/tags/kubernetes-develop/>#Kubernetes Develop</a></ul></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details><summary><span class=details>目录</span></summary><div class=inner><ul><li><a href=#overview aria-label=Overview>Overview</a><li><a href=#ipvs%e4%b8%8eiptables%e5%9c%a8kubernetes%e4%b8%ad%e5%ba%94%e7%94%a8%e6%97%b6%e7%9a%84%e9%97%ae%e9%a2%98 aria-label=IPVS与iptables在kubernetes中应用时的问题>IPVS与iptables在kubernetes中应用时的问题</a><li><a href=#%e5%a6%82%e6%9e%9c%e9%9c%80%e8%a6%81%e8%87%aa%e5%ae%9a%e4%b9%89proxier%e4%b8%ba%e4%bb%80%e4%b9%88%e4%bc%9a%e8%a7%a3%e5%86%b3%e8%bf%99%e4%b8%aa%e9%97%ae%e9%a2%98 aria-label=如果需要自定义proxier为什么会解决这个问题>如果需要自定义proxier为什么会解决这个问题</a><li><a href=#%e8%bd%af%e4%bb%b6%e7%9a%84%e8%ae%be%e8%ae%a1 aria-label=软件的设计>软件的设计</a><li><a href=#%e6%9c%80%e5%90%8e%e4%b8%80%e4%b8%aa%e9%97%ae%e9%a2%98 aria-label=最后一个问题>最后一个问题</a><li><a href=#%e5%ae%9e%e8%b7%b5%e5%9f%ba%e4%ba%8ehaproxy%e7%9a%84proxier aria-label=实践：基于haproxy的proxier>实践：基于haproxy的proxier</a><ul><li><a href=#%e6%bb%a1%e8%b6%b3%e6%9d%a1%e4%bb%b6 aria-label=满足条件>满足条件</a></ul><li><a href=#proxier%e4%b8%8eingress%e7%9a%84%e5%8c%ba%e5%88%ab aria-label=Proxier与Ingress的区别>Proxier与Ingress的区别</a></li></div></details></div></aside><script src=/js/pe-toc.min.445eb1bfc5e85dd13b9519fcc2a806522e9629b6224a2974052789ba00ab78af.js integrity="sha256-RF6xv8XoXdE7lRn8wqgGUi6WKbYiSil0BSeJugCreK8="></script><div class=post-content><blockquote><p>本文是关于Kubernetes service解析的第四章</p><ul><li><a href=https://cylonchau.github.io/kubernetes-service-controller.html target=_blank rel="noopener nofollow noreferrer">深入理解Kubernetes service - 你真的理解service吗?</a></li><li><a href=https://cylonchau.github.io/kubernetes-endpointslices.html target=_blank rel="noopener nofollow noreferrer">深入理解Kubernetes service - EndpointSlices做了什么？</a></li><li><a href=https://cylonchau.github.io/kubernetes-kube-proxy-code.html target=_blank rel="noopener nofollow noreferrer">深入理解Kubernetes service - kube-proxy架构分析</a></li><li>深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？</li></ul><p>所有关于Kubernetes service 部分代码上传至仓库 <a href=https://github.com/cylonchau/kube-haproxy target=_blank rel="noopener nofollow noreferrer">github.com/cylonchau/kube-haproxy</a></p></blockquote><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>在前两部分中，学习了一些 service,于kube-proxy在设计架构，但存在扩展问题将引入了一些问题：</p><ul><li>为什么需要了解这部分内容呢？</li><li>与传统架构有什么区别呢？</li><li>于eBPF 的 cilium又有什么区别呢？</li><li>既然eBPF可以做到，那为什么要这部分内容呢？</li></ul><p>接下来的内容将围绕这四个问题展开来讲，而不是代码的讲解，代码可以看置顶</p><h2 id=ipvs与iptables在kubernetes中应用时的问题>IPVS与iptables在kubernetes中应用时的问题<a hidden class=anchor aria-hidden=true href=#ipvs与iptables在kubernetes中应用时的问题>#</a></h2><p>对于在使用了kubernetes用户以及了解 kube-proxy 架构后，知道当集群规模过大时，service必将增多，而一个service未必是一条iptables/ipvs规则，对于kubernetes这种分布式架构来说，集群规模越大，集群状态就越不可控，尤其时kube-proxy。</p><p>为什么单指kube-proxy呢？想想可以知道，pod的故障 或 node 的故障对于kubernetes集群来说却不是致命的，因为 这些资源集群中存在 避免方案，例如Pod的驱逐。而kube-proxy或iptables/IPVS问题将导致服务的不可控 『抖动』例如规则生成的快慢和Pod就绪的快慢不一致，部分节点不存在 service 此时服务必然抖动。</p><p>再例如 iptables/IPVS 排查的难度对于普通运维工程师或开发工程师的技术水平有很高的要求，网上随处可见分析该类问题的帖子：</p><ul><li><p><a href="https://www.bilibili.com/video/BV1yK411V7oa/?spm_id_from=333.337.search-card.all.click&amp;vd_source=80a7f916d4f5b3fd494735dbc609331f" target=_blank rel="noopener nofollow noreferrer">kube-proxy源码分析与问题定位</a></p></li><li><p><a href=http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/%E5%8A%A0%E9%A4%9001%20%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90%EF%BC%9A%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%E6%B5%B7%E9%87%8FIPVS%E8%A7%84%E5%88%99%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%BD%91%E7%BB%9C%E5%BB%B6%E6%97%B6%E6%8A%96%E5%8A%A8%E9%97%AE%E9%A2%98%EF%BC%9F.md target=_blank rel="noopener nofollow noreferrer">案例分析：怎么解决海量IPVS规则带来的网络延时抖动问题？</a></p></li><li><p><a href=https://imroc.cc/kubernetes/networking/faq/ipvs-conn-reuse-mode.html#ipvs-%e8%bf%9e%e6%8e%a5%e5%a4%8d%e7%94%a8%e5%bc%95%e5%8f%91%e7%9a%84%e7%b3%bb%e5%88%97%e9%97%ae%e9%a2%98 target=_blank rel="noopener nofollow noreferrer">ipvs 连接复用引发的系列问题</a></p></li><li><p><a href=https://www.diva-portal.org/smash/get/diva2:1610208/FULLTEXT01.pdf target=_blank rel="noopener nofollow noreferrer">Investigating Causes of Jitter in Container Networking</a></p></li><li><p><a href=https://main.qcloudimg.com/raw/document/intl/product/pdf/457_37358_en.pdf target=_blank rel="noopener nofollow noreferrer">ContainerNative network LoadBalancer IPVS jitter</a></p></li></ul><p>对于上述问题，相信遇到了很难定位处理，虽然现在已fixed，并有eBPF技术的加入减少了此类问题的发生，但是eBPF实际同理于IPVS 都是需要对Linux内核有一定了解后才可以，这也就是为什么需要了解这部分</p><h2 id=如果需要自定义proxier为什么会解决这个问题>如果需要自定义proxier为什么会解决这个问题<a hidden class=anchor aria-hidden=true href=#如果需要自定义proxier为什么会解决这个问题>#</a></h2><p>这里就是放大到kubernetes意外的传统架构中，当直接部署于Linux系统上使用nginx等传统LB时就很少有人提到这些问题了，而这些问题存在一个关键字「Container」；而引发这个问题的则是 service。去除 service 的功能，传统架构于Kubernetes架构部署的应用则是相同的，只是区分了名称空间。</p><p>抓入关键的核心之后就做接下来的事情了，我称之为「shed kube-proxy, fetch service」；即把service提取到集群外部的LB之上，例如F5, nginx等。</p><p>这里会存在一个疑问：「这个不是ingress吗？」，这个问题会在下一章讲到 <a href=https://cylonchau.github.io/kubernetes-auditing.html target=_blank rel="noopener nofollow noreferrer">proxier与ingress有什么区别?</a></p><h2 id=软件的设计>软件的设计<a hidden class=anchor aria-hidden=true href=#软件的设计>#</a></h2><p>既然拿到了核心问题就该定义软件工作模式，这里将软件架构设计为三种：</p><ul><li>only fetch：任然需要 kube-proxy 组件，通过定义 contoller 将流量引入，即不过service，这种场景不会破坏现有的集群架构，从而去除service的功能，如果需要service功能配置外部service即可</li><li>SK (similar kube-proxy)：通过效仿kube-proxy + ipvs架构，将LB于proxier部署在每个worker节点上，让浏览都走本地</li><li>replacement kube-proxy：完全取代kube-proxy 这于cilium类似了，但不同的是，<strong>proxier</strong> 可以于 <code>kube-controller-manager</code>；<code>kube-scheduler</code> 作为控制平面为集群提供 <code>service </code>功能，而无需为所有worker节点都部署一个 <code>kube-proxy</code> 或 <code>cilium</code> 这种架构</li></ul><h2 id=最后一个问题>最后一个问题<a hidden class=anchor aria-hidden=true href=#最后一个问题>#</a></h2><p>此时可以引入最后一个问题了：「既然eBPF可以做到，那为什么要这部分内容呢？」。</p><p>答：其一简单，每个运维人员无需额外知识都可以对 service 问题进行排错，简便了运维复杂度。另外这一部分其实是对于完整企业生态来讲，统一的流量转发平台是所必须的，有了这个就不需要单独的 service 功能了</p><h2 id=实践基于haproxy的proxier>实践：基于haproxy的proxier<a hidden class=anchor aria-hidden=true href=#实践基于haproxy的proxier>#</a></h2><p>在扩展proxier需要对 kube-proxy 有一定的了解，并且，kube-proxy 在可扩展性来说做的也是相当不错的，我们只需要实现一个 proxier.go 就可以基本上完成了对 kube-proxy ；而 proxier.go 的核心方法只需要三个函数即可（==这里是根据iptables/ipvs的设计进行的，也可以整合为一个方法==）</p><p>除了这三个函数外，其他的函数全都是 kube-proxy 已经实现好的通用的，这里直接使用或者按照其他内置proxier的方法即可</p><h3 id=满足条件>满足条件<a hidden class=anchor aria-hidden=true href=#满足条件>#</a></h3><ul><li>haproxy工作与proxier相同的节点，可以是集群内也可以是集群外，整个集群只需要一个</li><li>实现方法：syncProxyRules(), syncService(), syncEndpoint()</li></ul><p>查看当前的service</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ kubectl get endpointslices
</span></span><span class=line><span class=cl>NAME           ADDRESSTYPE   PORTS   ENDPOINTS                     AGE
</span></span><span class=line><span class=cl>kubernetes     IPv4          <span class=m>6443</span>    10.0.0.4                      195d
</span></span><span class=line><span class=cl>netbox-l489z   IPv4          <span class=m>80</span>      192.168.1.241,192.168.1.242   2d1h</span></span></code></pre></td></tr></table></div></div></div></div><p>查看service 配置</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>yaml</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>netbox</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>clusterIP</span><span class=p>:</span><span class=w> </span><span class=m>192.168.129.5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>88</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>protocol</span><span class=p>:</span><span class=w> </span><span class=l>TCP</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>targetPort</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>sessionAffinity</span><span class=p>:</span><span class=w> </span><span class=l>None</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>ClusterIP</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>status</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>loadBalancer</span><span class=p>:</span><span class=w> </span>{}</span></span></code></pre></td></tr></table></div></div></div></div><p>通过 proxier 生成了对应的 backend 与 frontend，这样就可以通过 haproxy 作为一个外部LB来跨过 service 与 IPVS/IPTables，通过这种情况下，我们可以将集群拉出一个平面至传统架构上，而又不影响集群的功能</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-20230226234851398.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-20230226234851398.png#center alt=image-20230226234851398 onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><p>在这种场景下需要注意的是：</p><ul><li>OF模式下，我们需要 kube-proxy 组件，而使用 kube-proxy 组件</li><li>所有模式下，haproxy worker和kubernetes nodes需处于一个网络平面</li><li>非OF模式下需要自行修改 <code>kube-apiserver</code> 源代码（主要是使kubernetes service分配机制）</li></ul><h2 id=proxier与ingress的区别>Proxier与Ingress的区别<a hidden class=anchor aria-hidden=true href=#proxier与ingress的区别>#</a></h2><p>肯定有人会问，kubernetes提供了Ingress功能不是和这个一样吗？</p><p>答：对比一个LB是Proxier还是Ingress最好的区别就是“舍去kube-proxy”可以工作正常吗？</p><p>而kubernetes官方也提供说明，Ingress的后端是service，service的后端则是IPVS/IPTables，而IPVS的后端才是Pod；相对于Proxier LB，他的后端则直接是Pod，跨越了Service。</p><ul><li>Kubernetes Ingress 架构说明 <sup><a href=#1>[1]</a></sup></li><li>Traefik Ingress 架构说明 <sup><a href=#2>[2]</a></sup></li><li>APISIX Ingress 架构说明 <sup><a href=#3>[3]</a></sup></li></ul><p>而相对的本文的学习思路，haproxy官方提供了对应的解决方案 <sup><a href=#4>[4]</a></sup> ；而由此，可以灵活的为Kubernetes提供更多的LB方案</p><blockquote><p><strong>Reference</strong></p><p><sup id=1>[1]</sup> <a href=https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress target=_blank rel="noopener nofollow noreferrer"><em>Kubernetes Ingress 架构说明</em></a></p><p><sup id=2>[2]</sup> <a href=https://traefik.io/solutions/kubernetes-ingress/#architecture target=_blank rel="noopener nofollow noreferrer"><em>Traefik Ingress 架构说明</em></a></p><p><sup id=3>[3]</sup> <a href=https://github.com/apache/apisix-ingress-controller target=_blank rel="noopener nofollow noreferrer"><em>APISIX Ingress 架构说明</em></a></p><p><sup id=4>[4]</sup> <a href=https://haproxy-ingress.github.io/docs/examples/external-haproxy/ target=_blank rel="noopener nofollow noreferrer"><em>External haproxy</em></a></p></blockquote></div><div class=pe-copyright><hr><blockquote><p>本文为原创内容，版权归作者所有。如需转载，请在文章中声明本文标题及链接。</p><p>文章标题：深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？</p><p>文章链接：<a href=https://www.oomkill.com/2023/02/ch23-extend-kube-proxy/ target=_blank>https://www.oomkill.com/2023/02/ch23-extend-kube-proxy/</a></p><p>许可协议：<a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></p></blockquote></div><div class=comments-separator></div><h3 class=relatedContentTitle>相关阅读</h3><ul class=relatedContent><li><a href=/2023/02/ch19-kube-proxy-code/><span>深入理解Kubernetes service - kube-proxy架构分析</span></a></li><li><a href=/2023/02/ch17-service-controller/><span>深入理解Kubernetes service - 你真的理解service吗？</span></a></li><li><a href=/2022/11/ch34-auditing/><span>深入理解Kubernetes 4A - Audit源码解析</span></a></li><li><a href=/2022/11/ch32-authorization/><span>深入理解Kubernetes 4A - Authorization源码解析</span></a></li><li><a href=/2022/11/ch31-authentication/><span>深入理解Kubernetes 4A - Authentication源码解析</span></a></li></ul><div class=comments-separator></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.oomkill.com/tags/kubernetes/>Kubernetes</a></li><li><a href=https://www.oomkill.com/tags/kubernetes-develop/>Kubernetes Develop</a></li></ul><nav class=paginav><a class=prev href=https://www.oomkill.com/2023/02/ch18-endpointslices/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></polyline></svg>&nbsp;</span>
<span>深入理解Kubernetes service - EndpointSlices做了什么？</span>
</a><a class=next href=https://www.oomkill.com/2023/02/ch19-kube-proxy-code/><span class=title></span>
<span>深入理解Kubernetes service - kube-proxy架构分析&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span></a></nav></footer><div class=pe-comments-decoration><p class=pe-comments-title></p><p class=pe-comments-subtitle></p></div><div id=pe-comments></div><script src=/js/pe-go-comment.min.86a214102576ba5f9b7bdc29eed8d58dd56e34aef80b3c65c73ea9cc88443696.js integrity="sha256-hqIUECV2ul+be9wp7tjVjdVuNK74Czxlxz6pzIhENpY="></script><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="dark"?"dark":"light",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"cylonchau/blogs","data-repo-id":"R_kgDOIRlNSQ","data-category":"Announcements","data-category-id":"DIC_kwDOIRlNSc4CXy1U","data-mapping":"pathname","data-term":"posts/ch23 extend kube-proxy","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":getStoredTheme(),"data-lang":"zh-TW","data-loading":"lazy",crossorigin:"anonymous",async:""},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#pe-comments").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2025 <a href=https://www.oomkill.com/>Cylon's Collection</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> on
<a href=https://pages.github.com/ rel=noopener target=_blank>GitHub Pages</a> & Theme
        <a href=https://github.com/tofuwine/PaperMod-PE rel=noopener target=_blank>PaperMod-PE</a></span></footer><div class=pe-right-sidebar><a href=javascript:void(0); id=theme-toggle-float class=pe-float-btn><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</a><a href=#top class=pe-float-btn id=top-link><span id=pe-read-progress></span></a></div><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>